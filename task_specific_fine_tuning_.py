# -*- coding: utf-8 -*-
"""Task-Specific Fine-tuning .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exSY8KU5GvveopbDXDS4sciQd83i7azP
"""

!pip install transformers datasets accelerate evaluate

"""# ***Create Data***"""

from datasets import load_dataset

# Load tweet_eval sentiment dataset
dataset = load_dataset("tweet_eval", "sentiment")
print(dataset)

"""# ***Load tokenizer***"""

from transformers import AutoTokenizer

MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

"""# ***Tokenize the dataset***"""

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128          # Shorter tweets
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

"""# ***Load model***"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # tweet_eval sentiment: 0-negative,1-neutral,2-positive
    id2label={0: "negative", 1: "neutral", 2: "positive"},
    label2id={"negative": 0, "neutral": 1, "positive": 2}
)

"""# ***metrics (accuracy)***"""

import evaluate
import numpy as np

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

"""# ***Prepare training arguments***"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    metric_for_best_model="accuracy",
    logging_steps=10,
    report_to="none",
    fp16=True,  # Mixed precision if GPU available
)

"""# ***Trainer***"""

from transformers import Trainer, DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

"""# ***evaluate***"""

results = trainer.evaluate()
print(results)

"""# ***Saving***"""

trainer.save_model("./sentiment_model")
tokenizer.save_pretrained("./sentiment_model")

"""# ***Prediction***"""

from transformers import pipeline
import torch

def test_classifier():
    # Use GPU if available
    device = 0 if torch.cuda.is_available() else -1

    # Load the fine-tuned sentiment classifier
    classifier = pipeline(
        "text-classification",  # Correct pipeline type
        model="./sentiment_model",
        tokenizer="./sentiment_model",
        device=device
    )

    # Example test tweets
    test_reviews = [
        "This is the best product I've ever used!",
        "Absolutely terrible, waste of money.",
        "It's okay, nothing special.",
        "Amazing quality and fast shipping!",
        "Disappointed with the purchase.",
    ]

    print("\n" + "="*60)
    print("Testing Tweet Classifier")
    print("="*60 + "\n")

    # Run classifier on each review
    for review in test_reviews:
        result = classifier(review)[0]  # Returns a dict with 'label' and 'score'
        print(f"Review: {review}")
        print(f"â†’ Sentiment: {result['label']} (confidence: {result['score']:.2%})")
        print("-" * 60)

if __name__ == "__main__":
    test_classifier()

